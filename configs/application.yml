# =============================================================================
# Pycelize Application Configuration
# =============================================================================
# This configuration file contains all settings for the Pycelize Flask application
# specializing in Excel/CSV processing operations.

app:
  # Application metadata
  name: "Pycelize"
  version: "v0.0.1"
  description: "Professional Flask Application for Excel/CSV Processing"
  author: "Aris Nguyen"

  # Environment settings
  environment: "development" # development, staging, production
  debug: true

  # Server configuration
  host: "127.0.0.1"
  port: 5050

api:
  # API versioning
  version: "v1"
  prefix: "/api/v1"

  # API metadata for responses
  locale: "en_US"

  # Rate limiting (requests per minute)
  rate_limit: 100

  # Request timeout in seconds
  timeout: 300

file:
  # Upload settings
  upload_folder: "uploads"
  output_folder: "outputs"

  # Allowed file extensions
  allowed_extensions:
    - ".csv"
    - ".xlsx"
    - ".xls"

  # Maximum file size in MB
  max_file_size_mb: 50

  # Supported encodings for CSV files
  supported_encodings:
    - "utf-8"
    - "utf-8-sig"
    - "latin1"
    - "cp1252"
    - "iso-8859-1"

excel:
  # Default sheet name for new Excel files
  default_sheet_name: "Sheet1"

  # Info sheet name for metadata
  info_sheet_name: "Info"

  # Maximum column width for auto-adjustment
  max_column_width: 50

  # Include info sheet by default
  include_info_sheet: true

  # Auto-adjust column widths
  auto_adjust_columns: true

sql:
  # Supported database types
  supported_databases:
    - "postgresql"
    - "mysql"
    - "sqlite"

  # Default database type
  default_database: "postgresql"

  # Default batch size for SQL generation
  default_batch_size: 1000

  # Include transaction statements by default
  include_transaction: true

  # Include error handling by default
  include_error_handling: true

normalization:
  # Enable data normalization by default
  enabled: true

  # Backup original data before normalization
  backup_original: true

  # Generate normalization report
  generate_report: true

logging:
  # Log level: DEBUG, INFO, WARNING, ERROR, CRITICAL
  level: "DEBUG"

  # Log format
  format: "%(asctime)s - %(name)s - %(levelname)s - %(message)s"

  # Log file path (null for console only)
  file: "logs/pycelize.log"

  # Enable detailed logging
  detailed: true

# =============================================================================
# Chat Workflows Configuration
# =============================================================================
chat_workflows:
  # Enable/disable chat workflows feature
  enabled: true

  # Maximum concurrent WebSocket connections
  max_connections: 10

  # WebSocket server configuration
  websocket:
    host: "127.0.0.1"
    port: 5051
    ping_interval: 30
    ping_timeout: 10

  # Storage paths
  storage:
    workflows_path: "./automation/workflows" # Path to store workflow definitions
    dumps_path: "./automation/dumps" # Path to store dumps
    sqlite_path: "./automation/sqlite/chat.db" # SQLite database path

  # Backup configuration
  backup:
    enabled: true
    interval_minutes: 60 # Interval to check for backups
    snapshot_path: "./automation/sqlite/snapshots" # Path to store snapshots
    retention_days: 30 # Days to retain snapshots

  # Conversation partitioning
  partition:
    enabled: true
    strategy: "time-based" # Options: time-based, hash-based, folder-based
    subfolder_format: "%Y/%m" # Used for time-based partitioning

  # File handling
  file:
    max_upload_size_mb: 50 # Maximum upload size in MB
    allowed_extensions:
      - ".csv"
      - ".xlsx"
      - ".xls"
      - ".json"
    chunk_size_bytes: 8192 # 8 KB, for streaming uploads/downloads

  # Workflow execution
  execution:
    max_steps: 50 # Maximum steps per workflow
    step_timeout_seconds: 300 # Timeout per step in seconds
    stream_progress: true # Stream progress updates to client
    save_intermediate_files: true # Save intermediate files during execution

  # Dump and restore
  dump:
    compression: "gzip" # Options: gzip, zip, tar
    include_metadata: true # Include metadata in dumps
    include_logs: true # Include logs in dumps
